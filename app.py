import os
import json
import time
import re
from dataclasses import dataclass
from typing import List, Optional, Dict, Any, Tuple

import streamlit as st
import fitz  # PyMuPDF

from openai import OpenAI
from supabase import create_client, Client
from dotenv import load_dotenv


# =========================
# Config
# =========================

@dataclass
class Settings:
    openai_api_key: str
    supabase_url: str
    supabase_service_key: str
    storage_bucket: str = "manual-pages"

    # Retrieval
    top_k: int = 10

    # UI slider default = 0.00 (keep previous behavior)
    similarity_threshold: float = 0.00

    # Related pages
    max_related_pages: int = 5

    # Chunking
    chunk_size: int = 900
    chunk_overlap: int = 150

    # Models
    chat_model: str = "gpt-4.1-mini"
    embedding_model: str = "text-embedding-3-small"

    # text-embedding-3-small default dims
    embedding_dims: int = 1536


def _ensure_trailing_slash(url: str) -> str:
    url = (url or "").strip()
    if not url:
        return url
    return url if url.endswith("/") else (url + "/")

load_dotenv()
def load_settings() -> Settings:
    return Settings(
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        supabase_url=os.getenv("SUPABASE_URL"),
        supabase_service_key=os.getenv("SUPABASE_SERVICE_ROLE_KEY"),
    )

# =========================
# Clients
# =========================

@st.cache_resource
def get_openai_client(api_key: str) -> OpenAI:
    return OpenAI(api_key=api_key)


@st.cache_resource
def get_supabase_client(url: str, key: str) -> Client:
    return create_client(url, key)


# =========================
# Utilities
# =========================

def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:
    text = (text or "").strip()
    if not text:
        return []
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(start + chunk_size, n)
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        if end == n:
            break
        start = max(0, end - overlap)
    return chunks


def robust_json_loads(s: str) -> Optional[Dict[str, Any]]:
    try:
        return json.loads(s)
    except Exception:
        return None


def openai_embed(client: OpenAI, model: str, text: str) -> List[float]:
    resp = client.embeddings.create(model=model, input=text)
    return resp.data[0].embedding


def embedding_to_pgvector_str(emb: List[float]) -> str:
    return "[" + ",".join(f"{x:.8f}" for x in emb) + "]"


def is_refusal_answer(answer: str) -> bool:
    if not answer:
        return True
    return "ë¬¸ì„œì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" in answer.strip()


def merge_pages_cited_then_search(
    cited_pages: List[int],
    contexts: List[Dict[str, Any]],
    max_pages: int
) -> List[int]:
    """
    1) cited_pages ìš°ì„ 
    2) ë¶€ì¡±í•˜ë©´ ê²€ìƒ‰ ê²°ê³¼ contextsì—ì„œ ìœ ë‹ˆí¬ í˜ì´ì§€ë¡œ ë³´ì¶©
    3) ìµœì¢… í˜ì´ì§€ ì˜¤ë¦„ì°¨ìˆœ
    """
    picked: List[int] = []
    seen = set()

    for p in cited_pages or []:
        try:
            p = int(p)
        except Exception:
            continue
        if p in seen:
            continue
        seen.add(p)
        picked.append(p)
        if len(picked) >= max_pages:
            return sorted(picked)

    for c in contexts or []:
        p = int(c["page_number"])
        if p in seen:
            continue
        seen.add(p)
        picked.append(p)
        if len(picked) >= max_pages:
            break

    return sorted(picked)


def is_toc_page(text: str) -> bool:
    """
    ëª©ì°¨ í˜ì´ì§€ íœ´ë¦¬ìŠ¤í‹± íŒì • (í•œêµ­ì–´/ì˜ë¬¸ ëŒ€ì‘)
    - 'ëª©ì°¨' ë˜ëŠ” 'contents/table of contents' í¬í•¨ + ëª©ì°¨ íŠ¹ìœ  íŒ¨í„´(ë„íŠ¸ ë¦¬ë”/ì§§ì€ ë¼ì¸ ë°˜ë³µ/í˜ì´ì§€ë²ˆí˜¸ ë‚˜ì—´) ì¤‘ ì¼ë¶€
    """
    t = (text or "").strip()
    if not t:
        return False

    low = t.lower()

    keyword = ("ëª©ì°¨" in t) or ("table of contents" in low) or (re.search(r"\bcontents\b", low) is not None)
    if not keyword:
        return False

    dot_leader_count = len(re.findall(r"\.{3,}", t))

    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    numeric_tail_lines = 0
    for ln in lines[:80]:
        if re.search(r"\d+\s*$", ln) and (len(ln) < 120):
            numeric_tail_lines += 1

    short_lines = sum(1 for ln in lines[:80] if len(ln) <= 60)

    score = 0
    if dot_leader_count >= 3:
        score += 1
    if numeric_tail_lines >= 6:
        score += 1
    if short_lines >= 25:
        score += 1

    return score >= 1


def openai_answer_with_rag(
    client: OpenAI,
    model: str,
    question: str,
    contexts: List[Dict[str, Any]],
) -> Dict[str, Any]:
    """
    contexts: [{"page_number": int, "content": str, "similarity": float}, ...]
    return: {"answer": str, "cited_pages": [int, ...]}
    """
    ctx_lines = []
    for c in contexts:
        ctx_lines.append(f"[page={c['page_number']}, similarity={c['similarity']:.3f}]\n{c['content']}")
    ctx_text = "\n\n---\n\n".join(ctx_lines)

    system = (
        "ë„ˆëŠ” ì¥ë¹„ ë§¤ë‰´ì–¼ PDFë¥¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µí•˜ëŠ” ê³ ê°ì§€ì› ì±—ë´‡ì´ë‹¤.\n"
        "ê·œì¹™:\n"
        "1) ì•„ë˜ ì œê³µëœ 'ë§¤ë‰´ì–¼ ë°œì·Œ'ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë°˜ë“œì‹œ 'ë¬¸ì„œì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.' ë¼ê³  ë‹µí•˜ë¼.\n"
        "2) ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, ê°„ê²°í•˜ë˜ ì‚¬ìš©ìê°€ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ ë‹¨ê³„í˜•ìœ¼ë¡œ ì‘ì„±í•˜ë¼.\n"
        "3) ë‹µë³€ì— ê·¼ê±°ê°€ ëœ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ cited_pages ë°°ì—´ë¡œ ë°˜ë“œì‹œ í¬í•¨í•˜ë¼.\n"
        "4) ì¶œë ¥ì€ JSON í•˜ë‚˜ë¡œë§Œ: {\"answer\": string, \"cited_pages\": number[]}\n"
    )

    user = f"ì§ˆë¬¸:\n{question}\n\në§¤ë‰´ì–¼ ë°œì·Œ:\n{ctx_text}\n"

    try:
        resp = client.responses.create(
            model=model,
            input=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
        )
        text_out = resp.output_text
    except Exception:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.2,
        )
        text_out = resp.choices[0].message.content or ""

    data = robust_json_loads((text_out or "").strip())
    if not data or "answer" not in data:
        return {"answer": "ë¬¸ì„œì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", "cited_pages": []}

    raw_pages = data.get("cited_pages", [])
    cited_pages: List[int] = []
    for p in raw_pages:
        try:
            cited_pages.append(int(p))
        except Exception:
            pass
    cited_pages = sorted(set(cited_pages))

    return {"answer": str(data.get("answer", "")).strip(), "cited_pages": cited_pages}


# =========================
# Storage helpers
# =========================

def ensure_bucket_exists(sb: Client, bucket: str, public: bool = True) -> None:
    try:
        buckets = sb.storage.list_buckets()
        exists = any(b.get("name") == bucket for b in buckets)
        if not exists:
            sb.storage.create_bucket(bucket, public=public)
        return
    except Exception:
        pass

    try:
        sb.storage.create_bucket(bucket, public=public)
    except Exception:
        return


def supabase_upload_png(sb: Client, bucket: str, path: str, png_bytes: bytes) -> str:
    ensure_bucket_exists(sb, bucket, public=True)
    try:
        sb.storage.from_(bucket).upload(
            path=path,
            file=png_bytes,
            file_options={"content-type": "image/png", "upsert": "true"},
        )
    except Exception:
        ensure_bucket_exists(sb, bucket, public=True)
        sb.storage.from_(bucket).upload(
            path=path,
            file=png_bytes,
            file_options={"content-type": "image/png", "upsert": "true"},
        )
    return sb.storage.from_(bucket).get_public_url(path)


def _chunks(lst: List[str], n: int) -> List[List[str]]:
    return [lst[i:i + n] for i in range(0, len(lst), n)]


def delete_doc_and_assets(settings: Settings, doc_id: int) -> Dict[str, Any]:
    """
    doc_idì˜:
    - Storage(ì´ë¯¸ì§€) ì‚­ì œ
    - DB(rag_chunks, manual_pages, manual_docs) ì‚­ì œ
    """
    sb = get_supabase_client(settings.supabase_url, settings.supabase_service_key)

    # 1) ì´ë¯¸ì§€ path ìˆ˜ì§‘
    pages_res = (
        sb.table("manual_pages")
        .select("image_path")
        .eq("doc_id", doc_id)
        .execute()
    )
    image_paths = [r["image_path"] for r in (pages_res.data or []) if r.get("image_path")]

    # 2) Storage ì‚­ì œ (ë°°ì¹˜)
    storage_deleted = 0
    storage_failed: List[str] = []
    if image_paths:
        for batch in _chunks(image_paths, 100):
            try:
                sb.storage.from_(settings.storage_bucket).remove(batch)
                storage_deleted += len(batch)
            except Exception:
                storage_failed.extend(batch)

    # 3) DB ì‚­ì œ (ìˆœì„œ ì¤‘ìš”: child -> parent)
    # rag_chunks
    try:
        sb.table("rag_chunks").delete().eq("doc_id", doc_id).execute()
    except Exception as e:
        return {"ok": False, "error": f"rag_chunks delete failed: {e}", "storage_deleted": storage_deleted, "storage_failed": storage_failed}

    # manual_pages
    try:
        sb.table("manual_pages").delete().eq("doc_id", doc_id).execute()
    except Exception as e:
        return {"ok": False, "error": f"manual_pages delete failed: {e}", "storage_deleted": storage_deleted, "storage_failed": storage_failed}

    # manual_docs
    try:
        sb.table("manual_docs").delete().eq("id", doc_id).execute()
    except Exception as e:
        return {"ok": False, "error": f"manual_docs delete failed: {e}", "storage_deleted": storage_deleted, "storage_failed": storage_failed}

    return {"ok": True, "storage_deleted": storage_deleted, "storage_failed": storage_failed}


# =========================
# Ingest
# =========================

def ingest_pdf_to_supabase(settings: Settings, pdf_bytes: bytes, title: str) -> Tuple[int, int]:
    oai = get_openai_client(settings.openai_api_key)
    sb = get_supabase_client(settings.supabase_url, settings.supabase_service_key)

    doc_row = sb.table("manual_docs").insert({"title": title, "file_name": f"{title}.pdf"}).execute()
    doc_id = int(doc_row.data[0]["id"])

    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    total_chunks = 0

    for page_index in range(doc.page_count):
        page_number = page_index + 1
        page = doc.load_page(page_index)

        text = page.get_text("text") or ""
        toc_flag = is_toc_page(text)

        pix = page.get_pixmap(dpi=160)
        png = pix.tobytes("png")

        img_path = f"{doc_id}/page_{page_number:04d}.png"
        img_url = supabase_upload_png(sb, settings.storage_bucket, img_path, png)

        sb.table("manual_pages").upsert(
            {
                "doc_id": doc_id,
                "page_number": page_number,
                "image_path": img_path,
                "image_url": img_url,
                "is_toc": toc_flag,
            },
            on_conflict="doc_id,page_number",
        ).execute()

        chunks = chunk_text(text, settings.chunk_size, settings.chunk_overlap)
        if not chunks:
            continue

        rows = []
        for ci, chunk in enumerate(chunks):
            emb = openai_embed(oai, settings.embedding_model, chunk)
            if len(emb) != settings.embedding_dims:
                raise ValueError(f"Embedding dims mismatch: got {len(emb)}, expected {settings.embedding_dims}")

            rows.append(
                {
                    "doc_id": doc_id,
                    "page_number": page_number,
                    "chunk_index": ci,
                    "content": chunk,
                    "embedding": embedding_to_pgvector_str(emb),
                    "is_toc": toc_flag,
                }
            )
            total_chunks += 1
            if total_chunks % 60 == 0:
                time.sleep(0.25)

        sb.table("rag_chunks").insert(rows).execute()

    return doc_id, total_chunks


# =========================
# Retrieval
# =========================

def retrieve_contexts(
    settings: Settings,
    question: str,
    doc_id_filter: Optional[int] = None,
) -> Tuple[List[Dict[str, Any]], float]:
    oai = get_openai_client(settings.openai_api_key)
    sb = get_supabase_client(settings.supabase_url, settings.supabase_service_key)

    q_emb = openai_embed(oai, settings.embedding_model, question)
    if len(q_emb) != settings.embedding_dims:
        raise ValueError(f"Query embedding dims mismatch: got {len(q_emb)}, expected {settings.embedding_dims}")

    payload = {
        "query_embedding": embedding_to_pgvector_str(q_emb),
        "match_count": settings.top_k,
        "doc_id_filter": doc_id_filter,
    }

    # âœ… DB ë ˆë²¨ì—ì„œ is_toc=falseë§Œ ë°˜í™˜ë˜ë„ë¡ RPCê°€ í•„í„°ë§í•´ì•¼ í•©ë‹ˆë‹¤.
    res = sb.rpc("match_rag_chunks_v3", payload).execute()
    rows = res.data or []

    contexts = []
    top1_similarity = -1.0

    for i, r in enumerate(rows):
        sim = float(r.get("similarity", -1.0))
        if i == 0:
            top1_similarity = sim

        contexts.append(
            {
                "id": r["id"],
                "doc_id": r["doc_id"],
                "page_number": r["page_number"],
                "chunk_index": r["chunk_index"],
                "content": r["content"],
                "similarity": sim,
            }
        )

    return contexts, top1_similarity


def get_page_image_url(settings: Settings, doc_id: int, page_number: int) -> Optional[str]:
    sb = get_supabase_client(settings.supabase_url, settings.supabase_service_key)

    res = (
        sb.table("manual_pages")
        .select("image_url,is_toc")
        .eq("doc_id", doc_id)
        .eq("page_number", page_number)
        .limit(1)
        .execute()
    )
    if res.data:
        if bool(res.data[0].get("is_toc")) is True:
            return None
        return res.data[0].get("image_url")
    return None


def list_docs(settings: Settings) -> List[Dict[str, Any]]:
    sb = get_supabase_client(settings.supabase_url, settings.supabase_service_key)
    res = sb.table("manual_docs").select("id,title,created_at").order("created_at", desc=True).execute()
    return res.data or []


# =========================
# Streamlit UI
# =========================

st.set_page_config(page_title="PDF ë§¤ë‰´ì–¼ RAG ì±—ë´‡", layout="wide")
settings = load_settings()

st.title("ğŸ“˜ PDF ë§¤ë‰´ì–¼ RAG ì±—ë´‡ (Supabase + OpenAI)")

if not settings.openai_api_key or not settings.supabase_url or not settings.supabase_service_key:
    st.warning(
        "í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n\n"
        "- OPENAI_API_KEY\n"
        "- SUPABASE_URL\n"
        "- SUPABASE_SERVICE_ROLE_KEY\n"
    )
    st.stop()

mode = st.sidebar.radio("ë©”ë‰´", ["ê´€ë¦¬ì: PDF ì—…ë¡œë“œ/ì ì¬", "ì‚¬ìš©ì: ì±—ë´‡"])

st.sidebar.markdown("---")
settings.similarity_threshold = st.sidebar.slider(
    "Out-of-scope ìœ ì‚¬ë„ ì„ê³„ì¹˜(ë†’ì„ìˆ˜ë¡ ì—„ê²©)",
    min_value=0.00,
    max_value=1.00,
    value=float(settings.similarity_threshold),
    step=0.01,
    help="top1 similarityê°€ ì´ ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ 'ë¬¸ì„œì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'",
)


# -------------------------
# Admin
# -------------------------
if mode == "ê´€ë¦¬ì: PDF ì—…ë¡œë“œ/ì ì¬":
    st.subheader("ê´€ë¦¬ì: PDF ì—…ë¡œë“œ ë° RAG ì ì¬")

    title = st.text_input("ë¬¸ì„œ ì œëª©(ì˜ˆ: ì¥ë¹„A_ë§¤ë‰´ì–¼)", value="")
    pdf = st.file_uploader("PDF ì—…ë¡œë“œ", type=["pdf"])

    if st.button("ì ì¬ ì‹¤í–‰", type="primary", disabled=not (title and pdf)):
        with st.spinner("PDFë¥¼ í˜ì´ì§€ë³„ë¡œ ì²˜ë¦¬í•˜ê³ , ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ Supabaseì— ì €ì¥ ì¤‘..."):
            pdf_bytes = pdf.read()
            doc_id, total_chunks = ingest_pdf_to_supabase(settings, pdf_bytes, title)
        st.success(f"ì™„ë£Œ! doc_id={doc_id}, total_chunks={total_chunks}")
        st.info("â€» ëª©ì°¨ ì œì™¸(DBë ˆë²¨)ëŠ” is_toc íƒœê¹…ì´ í•„ìš”í•˜ë¯€ë¡œ, ì´ ë°©ì‹ ì ìš© í›„ì—ëŠ” ì¬ì ì¬ê°€ ë°˜ì˜ë©ë‹ˆë‹¤.")

    st.divider()
    st.subheader("ì ì¬ëœ ë¬¸ì„œ ëª©ë¡")
    docs = list_docs(settings)
    if not docs:
        st.info("ì•„ì§ ì ì¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for d in docs:
            st.write(f"- #{d['id']} | {d['title']} | {d['created_at']}")

    # âœ… ë¬¸ì„œ ì‚­ì œ UI
    st.divider()
    st.subheader("ë¬¸ì„œ ì‚­ì œ (DB + Storage ì´ë¯¸ì§€)")

    docs = list_docs(settings)
    if not docs:
        st.info("ì‚­ì œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        doc_map = {f"#{d['id']} - {d['title']}": int(d["id"]) for d in docs}
        sel_label = st.selectbox("ì‚­ì œí•  ë¬¸ì„œ ì„ íƒ", options=list(doc_map.keys()))
        del_doc_id = doc_map[sel_label]

        confirm = st.checkbox("ì •ë§ ì‚­ì œí•©ë‹ˆë‹¤. (DB + Storage ì´ë¯¸ì§€ê¹Œì§€ ì‚­ì œë¨)", value=False)
        if st.button("ì„ íƒ ë¬¸ì„œ ì‚­ì œ", type="secondary", disabled=not confirm):
            with st.spinner(f"doc_id={del_doc_id} ì‚­ì œ ì¤‘..."):
                result = delete_doc_and_assets(settings, del_doc_id)

            if result.get("ok"):
                st.success(f"ì‚­ì œ ì™„ë£Œ: doc_id={del_doc_id}")
                st.write(f"- Storage ì‚­ì œ: {result.get('storage_deleted', 0)}ê°œ")
                failed = result.get("storage_failed", [])
                if failed:
                    st.warning(f"Storage ì‚­ì œ ì‹¤íŒ¨ {len(failed)}ê°œ (ê¶Œí•œ/ê²½ë¡œ í™•ì¸ í•„ìš”)")
                    st.text("\n".join(failed[:50]))
            else:
                st.error(f"ì‚­ì œ ì‹¤íŒ¨: {result.get('error')}")

# -------------------------
# Chatbot
# -------------------------
else:
    st.subheader("ì‚¬ìš©ì: ë§¤ë‰´ì–¼ Q&A")

    docs = list_docs(settings)
    doc_options = [{"id": None, "title": "ì „ì²´ ë¬¸ì„œ(ëª¨ë“  ë§¤ë‰´ì–¼)"}] + [
        {"id": int(d["id"]), "title": f"#{d['id']} - {d['title']}"}
        for d in docs
    ]
    selected = st.selectbox(
        "ê²€ìƒ‰ ë²”ìœ„(ë¬¸ì„œ ì„ íƒ)",
        options=doc_options,
        format_func=lambda x: x["title"],
        index=0,
    )
    doc_id_filter = selected["id"]

    if "chat" not in st.session_state:
        st.session_state.chat = []

    for msg in st.session_state.chat:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    question = st.chat_input("ì¥ë¹„ ì‚¬ìš©/ì—ëŸ¬/ì„¤ì¹˜ ë°©ë²•ì„ ì§ˆë¬¸í•˜ì„¸ìš”...")

    if question:
        st.session_state.chat.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)

        with st.chat_message("assistant"):
            with st.spinner("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
                contexts, top1_similarity = retrieve_contexts(settings, question, doc_id_filter=doc_id_filter)
                st.caption(f"top1 similarity = {top1_similarity:.3f} (threshold={settings.similarity_threshold:.2f})")

                out_of_scope = (not contexts) or (top1_similarity < settings.similarity_threshold)

                cited_pages: List[int] = []

                if out_of_scope:
                    answer = "ë¬¸ì„œì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                else:
                    oai = get_openai_client(settings.openai_api_key)
                    out = openai_answer_with_rag(oai, settings.chat_model, question, contexts)
                    answer = out["answer"]
                    cited_pages = out.get("cited_pages", [])

                    if ("ë¬¸ì„œì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" not in answer) and (top1_similarity < (settings.similarity_threshold + 0.02)):
                        answer = "ë¬¸ì„œì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        cited_pages = []

                st.markdown(answer)

                if is_refusal_answer(answer):
                    related_pages = []
                    resolved_doc_id = None
                else:
                    related_pages = merge_pages_cited_then_search(
                        cited_pages=cited_pages,
                        contexts=contexts,
                        max_pages=settings.max_related_pages,
                    )

                    if doc_id_filter is not None:
                        resolved_doc_id = doc_id_filter
                    else:
                        resolved_doc_id = int(contexts[0]["doc_id"]) if contexts else None

                # âœ… ê´€ë ¨ í˜ì´ì§€ ìµœëŒ€ 5ì¥ (3 + 2 ë ˆì´ì•„ì›ƒ)
                if resolved_doc_id and related_pages:
                    st.caption("ê´€ë ¨ í˜ì´ì§€ (ìµœëŒ€ 5í˜ì´ì§€, í˜ì´ì§€ ìˆœ)")

                    # 1ì¤„: ìµœëŒ€ 3ê°œ
                    row1 = related_pages[:3]
                    cols1 = st.columns(3)
                    for idx in range(3):
                        with cols1[idx]:
                            if idx < len(row1):
                                p = row1[idx]
                                url = get_page_image_url(settings, resolved_doc_id, int(p))
                                if url:
                                    st.image(url, caption=f"p.{p}", width="stretch")
                                else:
                                    st.write(f"p.{p} ì´ë¯¸ì§€ ì—†ìŒ")

                    # 2ì¤„: ë‚˜ë¨¸ì§€(ìµœëŒ€ 2ê°œ)
                    row2 = related_pages[3:5]
                    if row2:
                        cols2 = st.columns(3)  # ê°€ìš´ë° ì •ë ¬ ëŠë‚Œ(2ê°œë§Œ ì“°ê³  1ê°œëŠ” ë¹„ì›€)
                        for idx in range(3):
                            with cols2[idx]:
                                if idx < len(row2):
                                    p = row2[idx]
                                    url = get_page_image_url(settings, resolved_doc_id, int(p))
                                    if url:
                                        st.image(url, caption=f"p.{p}", width="stretch")
                                    else:
                                        st.write(f"p.{p} ì´ë¯¸ì§€ ì—†ìŒ")

        st.session_state.chat.append({"role": "assistant", "content": answer})
